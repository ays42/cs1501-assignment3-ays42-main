    Each 4 of the LZW program implementations yielded very different results. In this short markdown paper, there will be breakdowns for each program along with an analysis of the performance tables that are provided within the folder "ass3 tables." After analyzing each program's compression results, we will look at each file's compression ratios through the 4 different tests and compare the results. Any files that failed to compress, compressed very poorly, or expanded will also receive further explanation.

    The first implementation that was tested was the given code that did not contain any modifications, and only used 12-bit codewords. After looking at the table, it's evident that 10 out of the 14 test files compressed successfully, while the other 4 test files ended up EXPANDING rather than compressing; these files were edit.exe (231KB/245KB), frosty.jpg (124KB/174KB), Lego-big.gif (92KB/126KB), and winnt256.bmp (154KB/156KB). While these 4 files did not compress, the other 10 files' compression ratios were very sporradic - for example, the 2960KB file "all.tar" had a compression ratio of 1.64 while the 68KB file "code.txt" had a compression ratio of 2.19. Along with this, the 1175KB file "large.txt" had a compression ratio of 2.005 while the 17KB file "gone_fishing.bmp" had a compression ratio of 1.7. After my analysis on the compression ratios of this implementation, I concluded that the file size did not contribute to a worse/better compression ratio; each file's contents most likely defined what the compression ratio was going to be. As for the files that expanded in this implementation, I will cover that issue later in this paper.

    The second implementation that was tested was the modified implementation that did not reset the codebook. This program was modified to take 9 to 16-bit codewords rather than only using 12-bit codewords. After testing each file through this implementation, it was clear that the resulting compression ratios indicated an improvement of performance. This implementation had 12 files successfully compress out of the 14, the two files that EXPANDED once again were frosty.jpg (124KB/168KB) and Lego-big.gif (92KB/120KB). Despite these files not compressing, it was clear from the graph that the bigger file sizes had better compression ratios; for example, all.tar (2960KB) had a 2.57 compression ratio, large.txt (1175KB) had a 2.295 compression ratio, and texts.tar (1350KB) had a 2.339 compression ratio. Along with this, the two files that expanded before but compressed this time (edit.exe and winnt256.bmp) had respective compression ratios of 1.55 and 2.48! It's clear that having a range of different bit-length codewords increased the effectiveness of the LZW algorithm, but how would that compare WITH a codebook reset?
    
    After testing the third implementation (which took 9 & 16-bit codewords AND reset the codebook), some files' compression ratios did not change, such as: assig2.doc, bmps.tar, code.txt, code2.txt, gone_fishing.bmp, medium.txt, wacky.bmp, and winnt256.bmp. However, 2 files' compression ratios improved: edit.exe, and large.txt. Therfore, a total of 10 files either compressed to the same size as the second implementation, or compressed to an even smaller size thanks to the codebook resetting. This, however, leaves 4 files left: frosty.jpg, Lego-big.gif, all.tar, and texts.tar. We know that the LZW algorithm has been struggling with the jpg and gif files, and in this implementation both files once again expanded - however, while Lego-big.gif's compression ratio did not change, frosty.jpg's actually expanded LESS than it did in the last implementation. Interestingly enough, all.tar and texts.tar had larger file sizes after compression, meaning it's possible that the codebook reset affects .tar files differently.

    Finally, the fourth implementation used the Unix 'compress' program, rather than LZW.java. The results this implementation yielded were really similar to both the second and third implementations - assig2.doc, bmps.tar, code.txt, code2.txt, gone_fishing.bmp, medium.txt, and winnt256.bmp had the same compression ratios. The file all.tar had a better compression ratio than the 3rd implementation, but was 1KB away from achieving the compression ratio that the 2nd implementation achieved. The file edit.exe and texts.tar had more successful compression ratios than the last two implementations, the file large.txt had a better compression ratio than the 2nd implementation, bit a worse compression ratio than the 3rd implementation. This leaves 3 files left, of course 2 of them being the .jpg and .gif files which had failed each compression implementation so far. While this Unix program technically contains the best coompression ratio results, it was unable to compress frosty.jpg and Lego-big.jpg at ALL (rather than expanding the file). I will speak on behalf of these files, as well as the wacky.bmp file in the next paragraph, as well on my thoughts on their compression results through the 4 tests.

    Since frosty.jpg and Lego-big.gif were the only two files that could NOT be compressed in any of the tests, I assumed that image files (such as .jpg/.jpeg, .png, and .gif) were unable to be compressed via LZW algorithms. Even though the code DID expand in the 2nd and 3rd implementations, they still did not grow an absurd amount, rather went from 124KB to ~ 160-168KB and 92KB to 120KB. This lead to compression ratios falling from 0.7381 to 0.775, which isn't necessarily terrible, but definitely not the result we're looking for when testing a compression algorithm. The final file I would like to take note on is wacky.bmp, as it had an QUESTIONABLY high compression ratio through all 4 tests - as a matter of fact, the only implementation that did not reach a compression ratio of 225.25 (901KB/4KB) was the first, which reached a compression ratio of 180.2 (901KB/5KB). That being said, it's interesting that this file was able to compress so well, especially when we used codewords that were varying in length, but I had to ask myself why that was, and why only that file was so successful? I personally was not able to decipher why this was, as I knew my code/implementation couldn't be wrong - I hope that a TA or Professor Khattab sees this (or I hope to ask a TA/Khattab one day about this particular file).